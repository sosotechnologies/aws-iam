# Dynamic Security

As part of our work publishing argo-workflow notifications to front end clients, we need to allow access to the MQTT broker from inside the cluster. The default behavior for a broker configured to operate without credentials is to listen on localhost (127.0.0.1). This necessitates configuring some for of security.  There are several options available as documented in the notes on [dynamic-security](https://mosquitto.org/documentation/dynamic-security/).

## deployment session

To configure the broker for dynamic security, we need to add two *plugin* lines to the configuration. The first is to specify the dynamic link library for the plugin, and the second of which is the location for a *dynamic-security.json* . . . 

Given the layout already used in the container image, this is simply the following.

```sh
sudo apt install mosquitto
```

```sh
sudo nano /etc/mosquitto/mosquitto.conf

# add below lines to it
plugin /usr/lib/mosquitto_dynamic_security.so
plugin_opt_config_file /etc/mosquitto/dynamic-security.json
```

We are able to generate an initial file using the following command.

```sh
sudo mosquitto_ctrl dynsec init /etc/mosquitto/dynamic-security.json admin-user
```

## obtain password from jsonfile and update 2-mosquitto-security.yaml
```sh
cat /etc/mosquitto/dynamic-security.json
```

"jWS9CUWdlOZPmxMtRapPmWcIHMF/ZA5Gte0fnLlSdFuybOk8qMjQyn8tEmU2Pm/uww6lVCvi9gMa0fH76u4muQ==",
          "salt":	"GTfgKV8qVZoZ6mSt",

<!-- ```sh
sudo systemctl restart mosquitto
``` -->

The operation of this command asks to set the credentials, which were generated by the following command.

```sh
echo mosquitto | md5sum | cut -f 1 -d ' '                            
```

The resulting initial configuration file was captured here as `dynamic-security.json` . . . 

## security configuration

The default configuration for access control list behavior is [documented](https://mosquitto.org/documentation/dynamic-security/) to be the following.

* publishClientSend: deny
* publishClientReceive: allow
* subscribe: deny
* unsubscribe: allow

### Broker side

These settings for these defaults can be seen (verified) using *mosquitto_ctrl* 
```sh
mosquitto_ctrl -u admin-user dynsec getDefaultACLAccess 
```
The defaults for those two access controls which deny actions can be changed as follows.  
```
mosquitto_ctrl -u admin-user dynsec setDefaultACLAccess publishClientSend allow
mosquitto_ctrl -u admin-user dynsec setDefaultACLAccess subscribe allow
```

## update the password and salt values in 2-mosquitto-security.yaml
update the values with the values obtained from command > mosquitto_ctrl dynsec init /mosquito/dynamic-security.json admin-user


## client side

We have a small deployment manifest *inflate-mqtt.yaml* which can be applied to the cluster into the default namespace, which will allow us to test naming across namespaces.
```
kubectl apply -f inflate-mqtt.yaml
```
Note, one must be in the *kubernetes/base/karpenter* directory to run the prior command.

from that pod, I can publish messages.
```
/ # mosquitto_pub -h mosquitto-mqtts.bda.svc.cluster.local -t worlds -m "hello"
/ # mosquitto_pub -h mosquitto-mqtts.bda -t worlds -m "there"
```

## subscriber running on broker side

Subscribing to the *worlds* topic gets the messages.

```
$ mosquitto_sub -h 172.20.175.242 -t worlds
hello
there
```

## broker logs

Smoke test / sanity check can be supported by checking the logs.

```
% kubectl logs -n bda mosquitto-846cf97cd6-bx6zr 
1711557845: mosquitto version 2.0.18 starting
1711557845: Config loaded from /mosquitto/config/mosquitto.conf.
1711557845: Loading plugin: /usr/lib/mosquitto_dynamic_security.so
1711557845: Opening ipv4 listen socket on port 1883.
1711557845: Opening ipv6 listen socket on port 1883.
1711557845: Opening websockets listen socket on port 9001.
1711557845: mosquitto version 2.0.18 running
1711558164: New connection from 10.144.134.202:54572 on port 1883.
```

# TODO: automate configuration of default control settings !

kubectl get nodes --show-labels
kubectl get pvc -n argo bda-alexandria-p1ab-pvc
kubectl describe pod source-pipeline-glvtj-s3-pull-video-1116346878 -n argo

kubectl get nodes --show-labels | grep "eks.amazonaws.com/nodegroup" 
kubectl get ec2nodeclass
kubectl get NodeClaim 
kubectl get nodepools
kubectl describe ec2nodeclass big-ebs-100g
kubectl get crd | grep karpenter
kubectl describe crd provisioners.karpenter.sh
k get no ip-10-128-168-10.ec2.internal --show-labels
kubectl get deployment -n karpenter karpenter -o=jsonpath='{.spec.template.spec.containers[0].image}'


kubectl get events -n argo --sort-by=.metadata.creationTimestamp

### main ussue
Failed Scheduling:

The pods are incompatible with multiple node pools (zone-gpu, zone-cpu, video-match-cpu, video-gpu, video-cpu, and shared-worker), primarily due to resource constraints and node affinity requirements.
The scheduling failures are caused by the specified resource requests (in this case, 7986m of CPU and 529Mi of memory) not being met by any available nodes.
Node Affinity and Selector Issues:

The logs mention that "5 node(s) didn't match Pod's node affinity/selector," meaning that the pods have specific affinity rules that restrict them to run on certain nodes, and none of the current nodes match these rules.
This is often defined in the pod specifications and might require adjusting the affinity settings or the configuration of the nodes themselves.


## create secret
If mosquitto is running or occupied by another port, you can know by 

```sh
sudo netstat -tuln | grep 1883
```

### You can stop the process:
```sh
sudo systemctl stop mosquitto
```
